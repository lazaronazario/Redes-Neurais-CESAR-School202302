{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnkA6fazG2Eh"
      },
      "source": [
        "# PyTorch: Variable, Gradientes e Grafo Computacional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdiT8nGeG2Ek"
      },
      "source": [
        "## Objetivos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDEX9Mk8G2Em"
      },
      "source": [
        "Este notebook introduz\n",
        "- o conceito de `Variables` do PyTorch,\n",
        "- uma interpretação numérica intuitiva do gradiente, e o\n",
        "- grafo computacional, utilizado para o cálculo automático do gradiente de uma função."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lPCM5vmG2Eo"
      },
      "source": [
        "Um dos principais fundamentos para que o PyTorch seja adequado para deep learning é a sua habilidade de\n",
        "calcular o gradiente automaticamente a partir da expressões definidas. Essa facilidade é implementada\n",
        "pelo tipo Variable do PyTorch, que adiciona ao tensor a facilidade de cálculo automático do gradiente pela construção dinâmica do grafo computacional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLATrVDrG2Eq"
      },
      "source": [
        "## Grafo computacional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbH9rnZmG2Er"
      },
      "source": [
        "```\n",
        "    y_pred = x * w\n",
        "    e = y_pred - y\n",
        "    e2 = e**2\n",
        "    J = e2.sum()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tPYkCldG2Es"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/vcasadei/images/master/GrafoComputacional.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnTsPEf6G2Eu"
      },
      "source": [
        "Variable possui 3 campos: o dado em si (data), o gradiente (grad) e um apontador (creator) para construir o grafo da backpropagation. Uma expressão utilizada para o cálculo do gradiente exige que todas suas expressões sejam calculadas com Variables, caso contrário não é possível construir o grafo computacional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wI-LZc6G2Ev"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/vcasadei/images/master/variables.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T14:09:19.148492",
          "start_time": "2017-11-24T14:09:18.096752"
        },
        "id": "OJt_zc0CG2Ex"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JONBAEwzG2E4"
      },
      "source": [
        "## Variable é criada a partir de um tensor e possui as mesmas funcionalidades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T14:09:19.305082",
          "start_time": "2017-11-24T14:09:19.150032"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT8CBLTaG2E6",
        "outputId": "09b51d1b-70ed-4bfd-ab44-243242f5acd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_t = 2 * torch.arange(0.,4.)\n",
        "y = Variable(y_t); y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T14:09:19.311537",
          "start_time": "2017-11-24T14:09:19.306712"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiSNffiPG2FD",
        "outputId": "7d58c75f-3094-4cc7-a377-72c5c0c2ea27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = Variable(torch.arange(0.,4.)); x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T14:09:19.318102",
          "start_time": "2017-11-24T14:09:19.313131"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW0pCsgaG2FK",
        "outputId": "a65902b9-608a-419d-a9e6-5184b56cca9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1.], requires_grad=True)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w = Variable(torch.ones(1),requires_grad=True); w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRXfFM2TG2FR"
      },
      "source": [
        "## Cálculo automático do gradiente da função perda J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFuPZhFGG2FU"
      },
      "source": [
        "Seja a expressão: $$ J = ((x  w) - y)^2 $$\n",
        "\n",
        "Queremos calcular a derivada de $J$ em relação a $w$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqNof23cG2FW"
      },
      "source": [
        "### Montagem do grafo computacional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T14:09:19.344631",
          "start_time": "2017-11-24T14:09:19.319779"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71lXNZ9mG2FX",
        "outputId": "e745a280-0234-49eb-bc79-6d814ef8ff01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(14., grad_fn=<SumBackward0>)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# predict (forward)\n",
        "y_pred = x * w\n",
        "\n",
        "# cálculo da perda J: loss\n",
        "e = y_pred - y\n",
        "e2 = e.pow(2)\n",
        "J = e2.sum()\n",
        "J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elNRuA34G2Fe"
      },
      "source": [
        "## Auto grad - processa o grafo computacional backwards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-10-04T15:55:45.308858",
          "start_time": "2017-10-04T15:55:45.304654"
        },
        "id": "y9mN5TqhG2Fm"
      },
      "source": [
        "O `backward()` varre o grafo computacional a partir da variável a ele associada e calcula o gradiente para todas as `Variables` que possuem o atributo `requires_grad` como verdadeiro.\n",
        "O `backward()` destroi o grafo após sua execução. Isso é intrínsico ao PyTorch pelo fato dele ser uma rede dinâmica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T14:09:19.355085",
          "start_time": "2017-11-24T14:09:19.346403"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrEXyjIDG2Fn",
        "outputId": "43ae91f3-c89a-4485-c559-bd5c588fcc1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-28.])\n"
          ]
        }
      ],
      "source": [
        "J.backward()\n",
        "print(w.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T14:09:19.363848",
          "start_time": "2017-11-24T14:09:19.356800"
        },
        "id": "MwkCb8LYG2Fu"
      },
      "outputs": [],
      "source": [
        "w.grad.data.zero_();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9biEyEt1G2Fz"
      },
      "source": [
        "## Interpretação do Gradiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVaH2WFyG2F0"
      },
      "source": [
        "O gradiente de uma variável final (J) com respeito à outra variável de entrada (w) pode ser interpretado como o quanto a variável final J vai aumentar se houver um pequeno aumento na variável de entrada (w).\n",
        "Por exemplo suponha que o gradiente seja 28. Isto significa se aumentarmos a variável w de 0.001, então J vai aumentar de 0.028."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T14:09:19.372040",
          "start_time": "2017-11-24T14:09:19.365927"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnO_ImuHG2F2",
        "outputId": "004c0554-ba4d-4416-e536-258a80500d64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(13.9720, grad_fn=<SumBackward0>)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eps = 0.001\n",
        "y_pred = x * (w+eps)\n",
        "J_new = (y_pred - y).pow(2).sum()\n",
        "J_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T14:09:19.403233",
          "start_time": "2017-11-24T14:09:19.373831"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ObOY_GnG2F8",
        "outputId": "52e7c3ac-8e97-4a4e-dd27-f8e2ef9ceacd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-0.027988434\n"
          ]
        }
      ],
      "source": [
        "print((J_new - J).data.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v625mnGlG2GC"
      },
      "source": [
        "## Back propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryb4BtcGG2GF"
      },
      "source": [
        "Uma forma equivalente explícita de calcular o gradiente é fazendo o processamento do backpropagation no grafo computacional, de forma explícita.\n",
        "Apenas como ilustração.\n",
        "\n",
        "Função de perda:\n",
        "$$ J(\\hat{y_i},y_i) = \\frac{1}{M} \\sum_{i=0}^{M-1} (\\hat{y_i} - y_i)^2 $$\n",
        "\n",
        "Gradiente:\n",
        "$$  \\mathbf{\\nabla{J_w}} = \\frac{2}{M}\\mathbf{x^T}(\\mathbf{x w^T} - \\mathbf{y}) $$\n",
        "\n",
        "Atualização dos parâmetros pelo gradiente descendente:\n",
        "$$ \\mathbf{w} = \\mathbf{w} − \\eta (\\mathbf{\\nabla J_w})^T $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T14:09:19.413681",
          "start_time": "2017-11-24T14:09:19.405014"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXrz6a0EG2GH",
        "outputId": "f046c029-f278-47fe-b6d7-cc6c1785c4b5",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n",
            "[1. 1. 1. 1.]\n",
            "[ 0. -2. -4. -6.]\n",
            "-28.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "dJ = 1.\n",
        "de2 = dJ * np.ones((4,))\n",
        "de = de2 * 2 * e.data.numpy()\n",
        "dy_pred = de\n",
        "dw = (dy_pred * x.data.numpy()).sum()\n",
        "print(dJ)\n",
        "print(de2)\n",
        "print(de)\n",
        "print(dw)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pvsJc0tG2GR"
      },
      "source": [
        "## Visualizando o grafo computacional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jg-tSOvCMs1",
        "outputId": "4c1c00fb-0dc5-4a13-c524-c5a6e1e231a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: graphviz in c:\\users\\jose.lazaro.n.silva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.20.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting git+https://github.com/szagoruyko/pytorchviz\n",
            "  Cloning https://github.com/szagoruyko/pytorchviz to c:\\users\\jose.lazaro.n.silva\\appdata\\local\\temp\\pip-req-build-c9q7couz\n",
            "  Resolved https://github.com/szagoruyko/pytorchviz to commit 0adcd83af8aa7ab36d6afd139cabbd9df598edb7\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: torch in c:\\users\\jose.lazaro.n.silva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchviz==0.0.2) (2.3.0)\n",
            "Requirement already satisfied: graphviz in c:\\users\\jose.lazaro.n.silva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchviz==0.0.2) (0.20.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\jose.lazaro.n.silva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz==0.0.2) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\jose.lazaro.n.silva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz==0.0.2) (4.10.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\jose.lazaro.n.silva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz==0.0.2) (1.12.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\jose.lazaro.n.silva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz==0.0.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\jose.lazaro.n.silva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz==0.0.2) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\jose.lazaro.n.silva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz==0.0.2) (2024.5.0)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\jose.lazaro.n.silva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz==0.0.2) (2021.4.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\jose.lazaro.n.silva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->torchviz==0.0.2) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\jose.lazaro.n.silva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->torchviz==0.0.2) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jose.lazaro.n.silva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch->torchviz==0.0.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\jose.lazaro.n.silva\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch->torchviz==0.0.2) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/szagoruyko/pytorchviz 'C:\\Users\\jose.lazaro.n.silva\\AppData\\Local\\Temp\\pip-req-build-c9q7couz'\n"
          ]
        }
      ],
      "source": [
        "!pip install graphviz\n",
        "!pip install git+https://github.com/szagoruyko/pytorchviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "VRZi7-urCZZS",
        "outputId": "ac890058-8ae4-4d79-ae3a-987b4c1d7aee"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (4) must match the size of tensor b (1000) at non-singleton dimension 1",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dot, make_dot_from_trace\n\u001b[1;32m----> 2\u001b[0m J \u001b[38;5;241m=\u001b[39m (\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m      3\u001b[0m p \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m:w} \u001b[38;5;66;03m# dicionário de parâmetros\u001b[39;00m\n\u001b[0;32m      4\u001b[0m out \u001b[38;5;241m=\u001b[39m make_dot(J,params\u001b[38;5;241m=\u001b[39mp)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (1000) at non-singleton dimension 1"
          ]
        }
      ],
      "source": [
        "from torchviz import make_dot, make_dot_from_trace\n",
        "J = ((w * x) - y).pow(2).sum()\n",
        "p = {'w':w} # dicionário de parâmetros\n",
        "out = make_dot(J,params=p)\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "bIqUBct4G2Gg"
      },
      "source": [
        "# Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEfWG-e5G2Gh"
      },
      "source": [
        "## Questões"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXLoZzF6G2Gj"
      },
      "source": [
        "1. Por que numa expressão computacional não é possível misturar `Variable` com tensores?\n",
        "\n",
        "Não é possível misturar Variable com tensores diretamente porque Variable contém metadados adicionais que permitem a construção do grafo computacional necessário para calcular os gradientes. Misturar Variable com tensores simples pode comprometer a integridade desse grafo, resultando em erros ou cálculos incorretos.\n",
        "\n",
        "2. O que acontece com o grafo computacional após a execução do `backward()`?\n",
        "\n",
        "Após a execução do backward(), o PyTorch calcula as derivadas parciais (gradientes) da função de perda em relação a cada um dos parâmetros da rede que possuem requires_grad=True. Nesse caso, para realizar múltiplas iterações de backpropagation, o grafo computacional precisa ser recriado a cada nova execução do forward pass. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayWukpAIG2Gk"
      },
      "source": [
        "## Atividades"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-10-06T22:02:17.474843Z",
          "start_time": "2017-10-06T22:02:17.468865Z"
        },
        "id": "k5Ayt5dAG2Gl"
      },
      "source": [
        "1. Execute um passo de atualização do valor de w, pelo\n",
        "gradiente descendente. Utilize um fator de aprendizado (*learning rate*) de 0.1\n",
        "para atualizar o `w`. Após, recalcule a função de perda:\n",
        "\n",
        "    - w = w - lr * w.grad.data\n",
        "    - execute a célula 1.3.1 e verifique o quanto que a perda J diminuiu - a célula 1.3.1 é a seguinte print((J_new - J).data.numpy())\n",
        "    \n",
        "2. No trecho abaixo, uma rede bastante conhecida, `resnet18` contendo 18 camadas\n",
        "   é criada, tendo\n",
        "   como entrada `xin` que é convertida para `Variable`, resultando na saída `y`.\n",
        "   \n",
        "   Descomente a linha que cria a vizualização do grafo computacional e execute a\n",
        "   célula para visualizar o grafo computacional da rede `resnet18`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dados iniciais\n",
        "y_t = 2 * torch.arange(0., 4.)\n",
        "y = Variable(y_t)\n",
        "x = Variable(torch.arange(0., 4.))\n",
        "w = Variable(torch.ones(1), requires_grad=True)\n",
        "\n",
        "# Previsão inicial\n",
        "y_pred = x * w\n",
        "\n",
        "# Cálculo da perda inicial J\n",
        "e = y_pred - y\n",
        "e2 = e.pow(2)\n",
        "J = e2.sum()\n",
        "\n",
        "# Backward para calcular gradientes\n",
        "J.backward()\n",
        "\n",
        "# Passo de atualização para w pelo gradiente descendente\n",
        "lr = 0.1\n",
        "w.data = w.data - lr * w.grad.data #isso que tu precisa\n",
        "\n",
        "# Zerando o gradiente acumulado\n",
        "w.grad.data.zero_()\n",
        "\n",
        "# Recalculando a previsão e a perda após a atualização\n",
        "y_pred_new = x * w\n",
        "J_new = (y_pred_new - y).pow(2).sum()\n",
        "\n",
        "# Calculando a mudança na perda\n",
        "print(\"Mudança na perda J:\", (J_new - J).data.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-24T14:09:19.519949",
          "start_time": "2017-11-24T16:09:18.148Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKwW1oQ8G2Go",
        "outputId": "4a442e65-d051-4307-b66c-8d5a6ab05425",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%pip install torchvision\n",
        "from torchvision import models\n",
        "xin = torch.randn(1,3,224,224)\n",
        "resnet18 = models.resnet18()\n",
        "y = resnet18(Variable(xin))\n",
        "g = make_dot(y, dict(resnet18.named_parameters()))\n",
        "g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S6srVMzG2G1"
      },
      "source": [
        "# Aprendizados com este notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "VyUo4FaAG2G2"
      },
      "source": [
        "Com tensores que têm requires_grad=True, o PyTorch consegue identificar as operações feitas nesses tensores e com isso calcular automaticamente os gradientes usando a diferenciação automática(autograd). Isso exclui a necessidade de calcular de forma manuaul os gradientes, simplificando o treinamento de modelos."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2.4-PyTorch_Variaveis_Gradientes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "5fe3e6f0cdaab8afdc61c52912fda83f7c0a71baaea1897dd7498e2df01e69ec"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
